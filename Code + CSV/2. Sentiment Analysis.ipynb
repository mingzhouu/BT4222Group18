{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb952c87",
   "metadata": {
    "id": "eb952c87"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import praw\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "import stanza\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from time import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score,  classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d977f",
   "metadata": {
    "id": "c83d977f"
   },
   "source": [
    "<h3>2.1. Data Cleaning and Splitting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jWUTFNhaUPYf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "jWUTFNhaUPYf",
    "outputId": "5b7aed47-3c20-4eb9-fb0a-0c0165769155"
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv('label1.csv')\n",
    "df_2 = pd.read_csv('label2.csv')\n",
    "df_3 = pd.read_csv('label3.csv')\n",
    "df_4 = pd.read_csv('label4.csv')\n",
    "df_5 = pd.read_csv('label5.csv')\n",
    "\n",
    "#merge all labelled dataframes\n",
    "df = pd.concat([df, df_2, df_3, df_4, df_5], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3d55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "pattern = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "#remove comments that are only hyperlinks\n",
    "df = df[~df['content'].str.contains(pattern)]\n",
    "#remove deleted comments\n",
    "df = df.loc[df['content'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145c4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stop words using the standard english set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    #Remove urls\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = url_pattern.sub('', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove None\n",
    "    tokens = list(filter(lambda x: x is not None, tokens))\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623d07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create new col for preprocessed text\n",
    "df['cleaned_content'] = df['content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b436c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split comments into sentences and do POS tagging\n",
    "sentences = [nltk.sent_tokenize(content) for content in df['cleaned_content']]\n",
    "pos_tagged_sentences = [[nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in comment] for comment in sentences]\n",
    "\n",
    "# For each comment, append a dictionary with the key being the tag and the values being the number of tokens belonging to the tag\n",
    "pos_features = []\n",
    "for content in pos_tagged_sentences:\n",
    "    pos_dict = {}\n",
    "    for sentence in content:\n",
    "        for word, tag in sentence:\n",
    "            pos_dict[tag] = pos_dict.get(tag, 0) + 1\n",
    "    pos_features.append(pos_dict)\n",
    "    \n",
    "# Convert POS features to a sparse matrix\n",
    "pos_vectorizer = DictVectorizer(sparse=False)\n",
    "pos_features = pos_vectorizer.fit_transform(pos_features)\n",
    "\n",
    "# Define a function for stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "#Initialize TFIDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words = 'english',\n",
    "    ngram_range = (1, 2), #Create ngrams of size 1 and 2\n",
    "    tokenizer = tokenize_and_stem,\n",
    "    min_df = 2, #ignore if ngram appears in less than 2 documents\n",
    "    max_df = 0.9 #ignore if ngram appears in more than 89% of the documents\n",
    ")\n",
    "\n",
    "#Vectorize the cleaned content using TFIDF\n",
    "tfidf_features = tfidf.fit_transform(df['cleaned_content'])\n",
    "\n",
    "#Vectorize the cleaned content using Bag of Words\n",
    "bow = CountVectorizer()\n",
    "bow_features = bow.fit_transform(df['cleaned_content'])\n",
    "\n",
    "#Combine all 3 features\n",
    "combined = np.concatenate((tfidf_features.toarray(), bow_features.toarray(), pos_features), axis = 1)\n",
    "\n",
    "#Add score\n",
    "combined = np.c_[combined, df['score']]\n",
    "\n",
    "#specify encoding sequence for label encoder\n",
    "sequence = ['neutral', 'negative', 'positive']\n",
    "\n",
    "#load label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "#encode labels\n",
    "labels = le.fit(sequence).transform(df['sentiment'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29f3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    combined,\n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4e84",
   "metadata": {},
   "source": [
    "<h3> 2.2. Random Forest Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5be7bcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5, estimator=RandomForestClassifier(random_state=42),\n",
       "              n_jobs=-1, random_state=42, scoring='f1_weighted',\n",
       "              search_spaces={'max_features': ['sqrt', 'log2'],\n",
       "                             'n_estimators': [30, 40, 50, 60, 70, 80, 90]})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize randomforest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "#Params dict for rf\n",
    "params_rf = {\n",
    "    'n_estimators': list(range(30, 100, 10)),\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "#Bayes search to find optimal param values using 5 fold cross validation\n",
    "bs_rf = BayesSearchCV(\n",
    "    n_jobs = -1,\n",
    "    estimator = rf,\n",
    "    search_spaces = params_rf,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_weighted',\n",
    "    random_state= 42\n",
    ")\n",
    "\n",
    "\n",
    "#fit\n",
    "bs_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f10997e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('max_features', 'sqrt'), ('n_estimators', 90)])\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  41                  66                  11\n",
      "true negative                  5                 206                  17\n",
      "true positive                  2                  40                  86\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.35      0.49       118\n",
      "           1       0.66      0.90      0.76       228\n",
      "           2       0.75      0.67      0.71       128\n",
      "\n",
      "    accuracy                           0.70       474\n",
      "   macro avg       0.76      0.64      0.66       474\n",
      "weighted avg       0.73      0.70      0.68       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {bs_rf.best_params_}\")\n",
    "\n",
    "#score\n",
    "rf_pred = bs_rf.predict(X_test)\n",
    "\n",
    "#confusion Matrix\n",
    "cf_rf = pd.DataFrame(\n",
    "        confusion_matrix(y_test, rf_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf_rf)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76456ab5",
   "metadata": {},
   "source": [
    "<h3> 2.3. Logistic Regression Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b7b0297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('C', 0.1)])\n",
      "[penalty type - none] f1 train: 0.6285335763611025 | f1 test: 0.5148194539186893\n",
      "Best parameters: OrderedDict([('C', 10.0)])\n",
      "[penalty type - l1] f1 train: 0.9968357708158168 | f1 test: 0.6724005499914476\n",
      "Best parameters: OrderedDict([('C', 100.0)])\n",
      "[penalty type - l2] f1 train: 0.6889529777686051 | f1 test: 0.5492880375397172\n"
     ]
    }
   ],
   "source": [
    "penalty_pairs = {\n",
    "    'none': 'none',\n",
    "    'l1': 'liblinear',\n",
    "    'l2': 'lbfgs'\n",
    "}\n",
    "\n",
    "#define parameters\n",
    "params_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "best_model_lr = \"\"\n",
    "best_f1 = 0\n",
    "\n",
    "#iterate through each penalty and solver pair\n",
    "for p, s in penalty_pairs.items():\n",
    "    if p == 'none':\n",
    "        lr = LogisticRegression(penalty = 'none')\n",
    "    else:\n",
    "        lr = LogisticRegression(penalty = p, solver = s)\n",
    "        \n",
    "    #Use bayes search to find optimal param values\n",
    "    bs_lr = BayesSearchCV(\n",
    "        n_jobs = -1,\n",
    "        estimator = lr,\n",
    "        search_spaces = params_lr,\n",
    "        cv = 5,\n",
    "        scoring = 'f1_weighted',\n",
    "        random_state= 42\n",
    "    )\n",
    "\n",
    "    bs_lr.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and test F1 score\n",
    "    print(f\"Best parameters: {bs_lr.best_params_}\")\n",
    "\n",
    "\n",
    "    #score\n",
    "    lr_pred = bs_lr.predict(X_test)\n",
    "    lr_pred_train = bs_lr.predict(X_train)\n",
    "    \n",
    "    f1_test = f1_score(y_test, lr_pred, average = 'weighted')\n",
    "    f1_train = f1_score(y_train, lr_pred_train, average = 'weighted')\n",
    "\n",
    "    print(f\"[penalty type - {p}] f1 train: {f1_train} | f1 test: {f1_test}\")\n",
    "    \n",
    "    if f1_train > best_f1:\n",
    "        best_model_lr = bs_lr\n",
    "        best_f1 = f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4846b579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('C', 10.0)])\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  55                  46                  17\n",
      "true negative                 20                 183                  25\n",
      "true positive                 14                  30                  84\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.47      0.53       118\n",
      "           1       0.71      0.80      0.75       228\n",
      "           2       0.67      0.66      0.66       128\n",
      "\n",
      "    accuracy                           0.68       474\n",
      "   macro avg       0.66      0.64      0.65       474\n",
      "weighted avg       0.67      0.68      0.67       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Clearly, L1 penalty gives the best results\n",
    "print(f\"Best parameters: {best_model_lr.best_params_}\")\n",
    "\n",
    "#score\n",
    "lr_pred = best_model_lr.predict(X_test)\n",
    "\n",
    "#confusion Matrix\n",
    "cf_lr = pd.DataFrame(\n",
    "        confusion_matrix(y_test, lr_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf_lr)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9a9e12",
   "metadata": {},
   "source": [
    "<h3> 2.4. XGBoost Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879c26ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize XGB classifier with 100 estimators\n",
    "xgb = XGBClassifier(n_estimators = 100, learning_rate = 0.1)\n",
    "\n",
    "#Params for bayes search\n",
    "params_xgb = {\n",
    "    'max_depth': Integer(3, 20),\n",
    "    'reg_alpha': Real(1e-9, 1.0, prior='log-uniform'),\n",
    "}\n",
    "\n",
    "#Bayes Search\n",
    "bs_xgb = BayesSearchCV(\n",
    "    n_jobs = -1,\n",
    "    estimator = xgb,\n",
    "    search_spaces = params_xgb,\n",
    "    cv = 5,\n",
    "    scoring = 'f1_weighted',\n",
    "    random_state= 42\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e5d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('max_depth', 19), ('reg_alpha', 0.0020211241974354348)])\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  53                  53                  12\n",
      "true negative                 20                 195                  13\n",
      "true positive                  9                  30                  89\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.45      0.53       118\n",
      "           1       0.70      0.86      0.77       228\n",
      "           2       0.78      0.70      0.74       128\n",
      "\n",
      "    accuracy                           0.71       474\n",
      "   macro avg       0.71      0.67      0.68       474\n",
      "weighted avg       0.71      0.71      0.70       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {bs_xgb.best_params_}\")\n",
    "\n",
    "#score\n",
    "xgb_pred = bs_xgb.predict(X_test)\n",
    "\n",
    "#confusion Matrix\n",
    "cf_xgb = pd.DataFrame(\n",
    "        confusion_matrix(y_test, xgb_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf_xgb)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd1f95",
   "metadata": {},
   "source": [
    "<h3> 2.5. MLP Tuning </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f65ae45f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=MLPClassifier(learning_rate='adaptive',\n",
       "                                      random_state=42),\n",
       "              n_jobs=-1, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh', 'logistic'],\n",
       "                             'hidden_layer_sizes': [10, 20, 30, 60, 120]})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize MLP classifier\n",
    "mlp = MLPClassifier(random_state=42, learning_rate = 'adaptive')\n",
    "\n",
    "#Params for bayes search\n",
    "params_mlp = {\n",
    "    'hidden_layer_sizes': [10, 20, 30, 60, 120],\n",
    "    'activation': ['relu', 'tanh', 'logistic']\n",
    "}\n",
    "\n",
    "#Bayes Search\n",
    "bs_mlp = BayesSearchCV(\n",
    "    estimator = mlp, \n",
    "    search_spaces = params_mlp, \n",
    "    cv=5, \n",
    "    n_jobs = -1, \n",
    "    random_state= 42\n",
    ")\n",
    "\n",
    "bs_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4bf2f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: OrderedDict([('activation', 'logistic'), ('hidden_layer_sizes', 20)])\n",
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  61                  37                  20\n",
      "true negative                 36                 163                  29\n",
      "true positive                 14                  28                  86\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       118\n",
      "           1       0.71      0.71      0.71       228\n",
      "           2       0.64      0.67      0.65       128\n",
      "\n",
      "    accuracy                           0.65       474\n",
      "   macro avg       0.63      0.63      0.63       474\n",
      "weighted avg       0.65      0.65      0.65       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and test F1 score\n",
    "print(f\"Best parameters: {bs_mlp.best_params_}\")\n",
    "\n",
    "#score\n",
    "mlp_pred = bs_mlp.predict(X_test)\n",
    "\n",
    "#confusion Matrix\n",
    "cf_mlp = pd.DataFrame(\n",
    "        confusion_matrix(y_test, mlp_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "print(cf_mlp)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, mlp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5762d",
   "metadata": {},
   "source": [
    "<h3> 2.6. Compare against Pre-tuned Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c835349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare with VADER\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Train test split using the same random state to ensure consistency\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8c06a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.29096832991748856\n"
     ]
    }
   ],
   "source": [
    "#List to store VADER output\n",
    "vader_labels = []\n",
    "\n",
    "#Initialize analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "#Iterate through X_test and label each comment/post\n",
    "for i, comment in enumerate(X_test):\n",
    "    sent = analyzer.polarity_scores(comment)\n",
    "    compound_score = sent['compound']\n",
    "    \n",
    "    #'Neutral' comments are from -0.05 to 0.05 non-inclusive; \n",
    "    #-1 is very negative, 0 is very neutral, 1 is very positive\n",
    "    if compound_score > 0.05:\n",
    "        sent = 2\n",
    "    elif compound_score < -0.05:\n",
    "        sent = 1\n",
    "    else:\n",
    "        sent = 0\n",
    "    \n",
    "    #Store output\n",
    "    vader_labels.append(sent)\n",
    "    \n",
    "#View weighted f1 score\n",
    "print(f\"Test F1 score: {f1_score(y_test, vader_labels, average = 'weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fb8a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                   7                  26                  15\n",
      "true negative                 93                  52                 159\n",
      "true positive                 33                   7                  82\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.15      0.08        48\n",
      "           1       0.61      0.17      0.27       304\n",
      "           2       0.32      0.67      0.43       122\n",
      "\n",
      "    accuracy                           0.30       474\n",
      "   macro avg       0.33      0.33      0.26       474\n",
      "weighted avg       0.48      0.30      0.29       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "cf_vader = pd.DataFrame(\n",
    "        confusion_matrix(y_test, vader_labels),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "\n",
    "print(cf_vader)\n",
    "\n",
    "#classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, vader_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "128e19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare with Stanza's sentiment analyzer\n",
    "#Train test split using the same random state once again\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], \n",
    "    labels, \n",
    "    test_size = 0.2, \n",
    "    random_state= 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08639684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 13:01:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d381332ebd7c49d496a8049d65fc709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 13:01:18 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2023-04-10 13:01:18 INFO: Use device: gpu\n",
      "2023-04-10 13:01:18 INFO: Loading: tokenize\n",
      "2023-04-10 13:01:21 INFO: Loading: sentiment\n",
      "2023-04-10 13:01:21 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "st = stanza.Pipeline('en', processors='tokenize, sentiment')\n",
    "\n",
    "#Function to return sentiment using stanza\n",
    "def classify_sentiment(comment):\n",
    "    doc = st(comment)\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        if sentence.sentiment == 0:\n",
    "            return 1\n",
    "        elif sentence.sentiment == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "#Classify sentiment using stanza\n",
    "stanza_pred = [classify_sentiment(comment) for comment in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21e52dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted neutral  predicted negative  predicted positive\n",
      "true neutral                  16                  31                   1\n",
      "true negative                194                  74                  36\n",
      "true positive                 51                  15                  56\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.33      0.10        48\n",
      "           1       0.62      0.24      0.35       304\n",
      "           2       0.60      0.46      0.52       122\n",
      "\n",
      "    accuracy                           0.31       474\n",
      "   macro avg       0.43      0.35      0.32       474\n",
      "weighted avg       0.56      0.31      0.37       474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#confusion Matrix\n",
    "cf_stanza = pd.DataFrame(\n",
    "        confusion_matrix(y_test, stanza_pred),\n",
    "        index = ['true neutral', 'true negative', 'true positive'],\n",
    "        columns = ['predicted neutral', 'predicted negative', 'predicted positive']\n",
    "    )\n",
    "\n",
    "print(cf_stanza)\n",
    "\n",
    "#classification report\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, stanza_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976683ac",
   "metadata": {},
   "source": [
    "<h3> 3. Labelling the Main Dataset with the Best Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f4d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the posts collected\n",
    "main_df = pd.read_csv('top_posts_year.csv')\n",
    "\n",
    "#Repeat data cleaning process\n",
    "pattern = r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "main_df = main_df[~main_df['content'].str.contains(pattern)]\n",
    "main_df = main_df.loc[main_df['content'] != '[deleted]']\n",
    "main_df['cleaned_content'] = main_df['content'].apply(preprocess_text)\n",
    "\n",
    "#Repeat feature engineering process\n",
    "sentences = [nltk.sent_tokenize(content) for content in main_df['cleaned_content']]\n",
    "pos_tagged_sentences = [[nltk.pos_tag(nltk.word_tokenize(sentence)) for sentence in comment] for comment in sentences]\n",
    "pos_main = []\n",
    "for content in pos_tagged_sentences:\n",
    "    pos_dict = {}\n",
    "    for sentence in content:\n",
    "        for word, tag in sentence:\n",
    "            pos_dict[tag] = pos_dict.get(tag, 0) + 1\n",
    "    pos_main.append(pos_dict)\n",
    "pos_main = pos_vectorizer.transform(pos_main)\n",
    "tfidf_main = tfidf.transform(main_df['cleaned_content'])\n",
    "bow_main = bow.transform(main_df['cleaned_content'])\n",
    "combined_main = np.concatenate((tfidf_main.toarray(), bow_main.toarray(), pos_main), axis = 1)\n",
    "combined_main = np.c_[combined_main, main_df['score']]\n",
    "\n",
    "#Predict sentiment\n",
    "sents = []\n",
    "for i in bs_xgb.predict(combined_main):\n",
    "    sent = ''\n",
    "    if i == 0:\n",
    "        sent = 'neutral'\n",
    "    elif i == 1:\n",
    "        sent = 'negative'\n",
    "    else:\n",
    "        sent = 'positive'\n",
    "    sents.append(sent)\n",
    "\n",
    "#Create pred_sentiment column in main df\n",
    "main_df['pred_sentiment'] = sents\n",
    "\n",
    "#Save to csv\n",
    "#main_df.to_csv('output.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
